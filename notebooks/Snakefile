# Snakemake pipeline for generating reports about residues involved in 
# protein-protein interactions when you need to examine multiple pairs of
# proteins in various related structures. If you just have to look at pairs of 
# proteins in very few sets of
# related structures, then see my notebook 
# `Using PDBsum data to highlight changes in protein-protein interactions.ipynb` 
# that you can run by going to https://github.com/fomightez/pdbsum-binder
# Needs standard packages for Jupytext and Pandas installed via pip.
# The user has to provide a text table named `int_matrix.txt` where each line 
# corresponds to specifying the following with each item separated by a space:
# PDB_code#1 chain#1 chain#2 PDBcode#2 chain#1 chain#2
# 
# The text would be actual PDB codes and chain designations.
# The order of the chain designations after each structure's PDB code should 
# match , meaning the designation of `chain#1` following `PDB_code#1` 
# corresponds to the same `chain#1` after `PDBcode#2`, with similar arrangement 
# holding for chain#2. 
# To help claify that, the following code between the dashed lines
# can have the starting `#`s removed at the start of each line and that code can 
# be run in a Jupyter notebook to make such a table involving several 
# combinations for examining pairs of interactions involving chains 
# N, R, and K in PDB entries 6kiv, 6kix, and 6kiz:
#-------------------------
#s='''6kiv N R 6kiz N R
#6kiv N R 6kix N R
#6kiz N R 6kix N R
#6kiv K N 6kiz K N
#6kiv K N 6kix K N
#6kiv R K 6kiz R K
#6kiv R K 6kix R K
#'''
#%store s >int_matrix.txt
#-------------------------
# 
# 
# See the accompanying notebook entitled `Using snakemake to highlight changes in multiple protein-protein interactions via PDBsum data.ipynb` for a demo.
# This file can be run after making the text table `int_matrix.txt` by calling
# `!snakemake -j1` from inside a jupyter notebook or run via `snakemake` on the 
# command line.
# Via MyBinder, run this Snakefile with the following:
# !snakemake --cores 1
# Only 1 core, because I think when I was using 8 it would commonly cause a race
# where more than one notebook was getting auxillary scripts and overwriting as
# as the other notebooks was trying to use. More reliable with 1. But if it, did 
# fail when using more cores, try re-running again because it should just 
# complete the missing files.
# For cleaning, there won't be any conflicts, so use the following on MyBinder:
# !snakemake --cores 8 clean
# More general info:
# If you had a ton of comparisons to process and wanted to take advantage of 
# parallel processing in the snakemake run you can read this section:
# Initiate with `snakemake -j X` if the file is named `Snakefile`, 
# replacing the `X` with the number of cores available. Otherwise, initiate with 
# `snakemake -s <snakefile_name> --cores X` if the file is named any thing other
# than `Snakefile`, where `X` is the number of cores.
# To just initiate a rule/step, run something like:
# `snakemake -j 8 <name_of_rules>`, where the number 8 is replaced by the 
# result of the command `getconf _NPROCESSORS_ONLN`.


import os
import sys
import datetime
now = datetime.datetime.now()
import pandas as pd
import nbformat as nbf

# INPUT COMPARISON MATRIX/ TABLE------------------------------------------------
# User's provide the information as table for the comparisons to do. See above
# for the format of that.
# I'm bringing it in here so that I can generate the names of the notebooks that
# need to be made. So that I can use these in snakemake as output and later even
# as output.
# For now the file with the table must be named named `int_matrix.txt`.

table_file_to_use  = "int_matrix.txt" # name of the table with info on each
# row to make a Jupyter notebook with the reports
column_names=(["structure1","structure1_chain1","structure1_chain2",
    "structure2","structure2_chain1","structure2_chain2"])
df = pd.read_table(table_file_to_use, 
    names=column_names, index_col=None,  delim_whitespace=True)
# use that to define `nb_files` and `processed_nb_files`
prefix_to_use_for_report_nbs = "interactions_report_for_"
nb_files = []
py_files = []
#nb_files_without_py = []
for indx,row in df.iterrows():
    main = (f'{"_".join(row.tolist())}')
    nb_name = f"{prefix_to_use_for_report_nbs}{main}.py.ipynb" # leftover from 
    # when I was trying to use `notebook:` from snakemake to run a notebook
    nb_name_simpler = f"{prefix_to_use_for_report_nbs}{main}.ipynb"
    py_name = f"{prefix_to_use_for_report_nbs}{main}.py"
    nb_files.append(nb_name_simpler)
    py_files.append(py_name)
    #nb_files_without_py.append(nb_name_simpler)
unprocessed_nb_files = [f"unprocessed_{x}" for x in nb_files]





# FILES THAT WILL BE GENERATED--------------------------------------------------
# py_files #Python versions that are easier to past here that will be converted
# to the notebooks by jupytext
#nb_files # the run notebooks generated by running jupytext with the `py_files`
results_archive = f"interactions_report_nbs{now.strftime('%b%d%Y%H%M')}.tar.gz"#
#archive of processed notebooks for downloading






# Additional, special settings--------------------------------------------------
# note: I had originally considered using nbformat to create the cells of the 
# generated notebook like I recently did with the script powering my 
# `imgs2RISEslides` repo. However, it turns out to do the prep work there is a 
# lot of cells before the main scripts. And most importantly, the cell number 
# and cell content are standardized. In the case of `imgs2RISEslides` the cell 
# number and content were not standardized and it made sense to use nbformat to 
# make cells to be able to adjust amount of cells. For here, it makes more sense
# and is way more direct to use a SINGLE standardized template text file 
# (actually python script code underlying) and use Jupytext to convert it into a 
# notebook.

nb_stub_as_py='''# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.9.1
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# # Interactions of Chain structure1_chain1_PLACEHOLDER_TEXT and Chain structure1_chain2_PLACEHOLDER_TEXT in structure1_PLACEHOLDER_TEXT vs. Chain structure2_chain1_PLACEHOLDER_TEXT and Chain structure2_chain2_PLACEHOLDER_TEXT in structure2_PLACEHOLDER_TEXT

# ### Preparation

structure1 = "structure1_PLACEHOLDER_TEXT"
structure2 = "structure2_PLACEHOLDER_TEXT"

structure1_chain1 = "structure1_chain1_PLACEHOLDER_TEXT"
structure1_chain2 = "structure1_chain2_PLACEHOLDER_TEXT"
structure2_chain1 = "structure2_chain1_PLACEHOLDER_TEXT"
structure2_chain2 = "structure2_chain2_PLACEHOLDER_TEXT"

structure1_data_name = "datai_structure1_PLACEHOLDER_TEXT_structure1_chain1_PLACEHOLDER_TEXT_structure1_chain2_PLACEHOLDER_TEXT.txt"
structure2_data_name = "datai_structure2_PLACEHOLDER_TEXT_structure2_chain1_PLACEHOLDER_TEXT_structure2_chain2_PLACEHOLDER_TEXT.txt"


def get_protein_inter_data_files(pdb_code,chain1,chain2,output_file_name):
    #Takes a PDB entry accession identifier alphanumeic (PDB code), a chain 
    #identifier for chain 1 and chain identifier for chain 2, along with a
    #name to give the file produced when the data is retrieved and saved.
    #The proteins have to interact in the structure for meaningful data to be returned.
    source_url = "http://www.ebi.ac.uk/thornton-srv/databases/cgi-bin/pdbsum/GetIface.pl"
    # !curl -L -o {output_file_name} --data "pdb={pdb_code.lower()}&chain1={chain1}&chain2={chain2}" {source_url}
# Get data file for structure #1    
get_protein_inter_data_files(structure1,structure1_chain1,structure1_chain2,structure1_data_name)
# Get data file for structure #2   
get_protein_inter_data_files(structure2,structure2_chain1,structure2_chain2,structure2_data_name)

# Get a file if not yet retrieved / check if file exists
import os
file_needed = "similarities_in_proteinprotein_interactions.py"
if not os.path.isfile(file_needed):
    # !curl -OL https://raw.githubusercontent.com/fomightez/structurework/master/pdbsum-utilities/{file_needed}
file_needed = "differences_in_proteinprotein_interactions.py"
if not os.path.isfile(file_needed):
    # !curl -OL https://raw.githubusercontent.com/fomightez/structurework/master/pdbsum-utilities/{file_needed}

# ## Generate reports
# ### Similarities

# %run -i similarities_in_proteinprotein_interactions.py

# ### Differences

# %run -i differences_in_proteinprotein_interactions.py
'''
# ---End of Additional, special settings----------------------------------------



##----------------HELPER FUNCTIONS----------------------------------------------
def write_string_to_file(s, fn):
    '''
    Takes a string, `s`, and a name for a file & writes the string to the file.
    '''
    with open(fn, 'w') as output_file:
        output_file.write(s)







# SNAKEMAKE RULES---------------------------------------------------------------

rule all:
    input:
        results_archive

# ---------------Individual Rules---------------------------------------------

# Delete any generated files so can trigger full run easily after cleaning
'''
The `touch` commands added make sure files matching each and every pattern of
output so that the `rm` commands don't throw an error.
'''
rule clean:
    shell:
        '''
        touch interactions_report_nbs18199xlkleFAKE.tar.gz
        touch interactions_report_for_18199xlkleFAKE.ipynb
        touch interactions_report_for_18199xlkleFAKE.py
        touch datai_18199xlkleFAKE.txt
        touch pdbsum_prot_interactions_list_to_df.py
        touch differences_in_proteinprotein_interactions.py
        touch similarities_in_proteinprotein_interactions.py
        rm interactions_report_nbs*.tar.gz
        rm interactions_report_for_*.ipynb
        rm interactions_report_for_*.py
        rm datai_*.txt
        rm pdbsum_prot_interactions_list_to_df.py
        rm differences_in_proteinprotein_interactions.py
        rm similarities_in_proteinprotein_interactions.py
        '''


# Use the table & make a Python script that will be used later to make notebook
rule read_table_and_create_py:
    input: table_file_to_use
    output: py_files
    run:
        for indx,row in df.iterrows():
            info_tag= (f'{"_".join(row.tolist())}')
            py_file_name = f"{prefix_to_use_for_report_nbs}{info_tag}.py"
            stub_as_py = nb_stub_as_py # You cannot use an immutable string from
            # the main namespace in a rule if you are going to change it. If it 
            # remains unaltered, it works. The way around is to simply assign 
            # a new variable name within the rule. HAS TO BE DIFFERENT NAME.
            stub_as_py = stub_as_py.replace(
                "structure1_PLACEHOLDER_TEXT",row[0])
            stub_as_py = stub_as_py.replace(
                "structure1_chain1_PLACEHOLDER_TEXT",row[1])
            stub_as_py = stub_as_py.replace(
                "structure1_chain2_PLACEHOLDER_TEXT",row[2])
            stub_as_py = stub_as_py.replace(
                "structure2_PLACEHOLDER_TEXT",row[3])
            stub_as_py = stub_as_py.replace(
                "structure2_chain1_PLACEHOLDER_TEXT",row[4])
            stub_as_py = stub_as_py.replace(
                "structure2_chain2_PLACEHOLDER_TEXT",row[5])
            write_string_to_file(stub_as_py, py_file_name)


# In Jupyter I made a template notebook and then converted it to Python script
# that I thought I'd be able to paste into here because it worked in 
# `bendIt_analysis.py`.
# After pasted in here and the docstring in the function (see below) fixed, I 
# replaced the items that well be swapped in for the individual notebooks with 
# unique placeholders.
# Converted the template notebook `pdbsum_simple_report_template.ipynb` to a 
# Python script I can paste in here using AFTER DELETING A DOCSTRING for the
# `get_protein_inter_data_files` function
# so that the quotes didn't mess up the stub being a long docstring:
#!jupytext --to py pdbsum_simple_report_template.ipynb
# NOTES FOR USING JUPYTEXT IN THIS PROCESS
# To convery a script to a notebook without running it; help at 
# https://jupytext.readthedocs.io/en/latest/using-cli.html
# !jupytext --to notebook pdbsum_simple_report_template.py --output zzz.ipynb
# To convery a script to a notebook and run it at same time
#!jupytext --to notebook --execute pdbsum_simple_report_template.py --output zzz.ipynb




# Convert the python scripts to notebooks and run them
'''
I had planned to use the new feature of being able to run notebooks. See
https://github.com/snakemake/snakemake/tree/master/tests/test_jupyter_notebook 
and
https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#jupyter-notebook-integration
But I having major problems combining the use of wildcards with running the notebook,
plus it adds a preamble I need to run. Is it just easier to run with jupytext or nbcomvert?
This is what I tried before:
rule run_notebooks_to_generate_report:
    input: "{details}.py.ipynb"
    output: "processedwpreamble_{details}.py.ipynb"
    log:
        # optional path to the processed notebook
        notebook={output}
    notebook:
        {input}

Then I was going to use nbconvert but because jupytext allows me to save a
a script within a script (or Snakefile, in this case), I switched to that
to make a python script and then convert it. So this is what I had before 
switching from nbconvert:
rule run_notebooks_to_generate_report_using_nbconvert:
    input: "unprocessed_"+prefix_to_use_for_report_nbs+"{details}.ipynb"
    output: prefix_to_use_for_report_nbs+"{details}.ipynb"
    shell: '!jupyter nbconvert --to notebook --execute {input} --output {output}'

I also added after the conversion step removing the input python scripts to 
progress towards a cleaner interface where the generated notebooks are easier to
see.
'''
rule convert_scripts_to_nb_and_run_using_jupytext:
    input: prefix_to_use_for_report_nbs+"{details}.py"
    output: prefix_to_use_for_report_nbs+"{details}.ipynb"
    shell: 'jupytext --to notebook --execute {input} --output {output};rm {input}'




# Remove the snakemake preamble from the notebooks
'''
Snamemake adds a preamble cell when used to run the notebook. This will remove 
first cell from each notebook using nbformat.
THIS WAS ONLY NECESSARY WHEN I WAS HOPING TO USE SNAKEMAKES INTERNAL NOTEBNOOK
HANDLING TO RUN THEM BUT THE WILDCARDS WITH NOTEBOOKS PROVED MORE COMPLEX.
WHAT I HAD
rule remove_preamble_cell:
    input: "processedwpreamble_{details}.py.ipynb"
    output: "processed_{details}.ipynb"
    run:
        ntbk = nbf.read({input} nbf.NO_CONVERT)
        new_ntbk = ntbk
        new_ntbk.cells = ntbk.cells[1:]
        nbf.write({output}, version=nbf.NO_CONVERT)
'''



# Create archives with the executed nb_files
'''
I added some cleaning as well to remove the auxillary scripts and data to make
# it easier to see the generated notebooks among the files.
'''
rule make_archive:
    input: nb_files
    output: results_archive
    shell:
        '''
        rm datai_*.txt
        rm pdbsum_prot_interactions_list_to_df.py
        rm differences_in_proteinprotein_interactions.py
        rm similarities_in_proteinprotein_interactions.py
        tar -czf {output} {input}; echo "Be sure to download {output}."
        '''