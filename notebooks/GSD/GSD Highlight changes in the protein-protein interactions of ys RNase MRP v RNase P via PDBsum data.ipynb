{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSD Highlight changes in the protein-protein interactions of ys RNase MRP vs. RNase P via PDBsum data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an effort to adapt the generic notebook I made to look at protein-protein interactions in pairs of related structures to look at the combinations of protein-protein interactions for yeast RNase MRP vs. RNase P.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Step #1:** Make a table with a matrix of the protein-protein combinations in the pairs of cryo-EM structures of RNase MRP and RNase P.\n",
    "\n",
    "I used the PDBsum pages for protein-protein interactions make this table. I could have computationally generated the combinations; however, that way several will not be actually relevant and so instead of sorting out which ones actually return an 'empty' list of interactions for both structures from PDBsum (which shouldn't be too hard and could be added so down the road one could just supply two PDB code ids and let all the reports get generated, plus note that it has to be emptry for both structures because if one has none and the other has some like Pop1p[chain B] and Pop5p[chain E] that interact in Rnase P but not in RNase MRP, this is definitely important differences to catch; just to not the flip side of that is that in Rnase MRP Pop1p gains interactions with Rmp1[chain L] and Pop4p[Chain D] not seen in RNase P), I just decided to construct it myself so nothing is missed due to an error in handling the steps for doing it that way. Plus certain combinations were originally the impetus for this effort and so those were actuall added first and then I expanded out to check the other interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 's' (str) to file 'int_matrix.txt'.\n"
     ]
    }
   ],
   "source": [
    "s='''7c7a F G 6ah3 F G\n",
    "7c7a F B 6ah3 F B\n",
    "7c7a G B 6ah3 G B\n",
    "7c7a E B 6ah3 E B\n",
    "7c7a I B 6ah3 I B\n",
    "7c7a F I 6ah3 F I\n",
    "7c7a G E 6ah3 G E\n",
    "7c7a I E 6ah3 I E\n",
    "'''\n",
    "%store s >int_matrix.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step #2:** Move the Snakefile to process the table of interactions to this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../Snakefile ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step #3:** Run snakemake and it will process the `int_matrix.txt` file to extract the information and make individual notebooks corresponding to analysis of the interactions for each line.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mUsing shell: /bin/bash\u001b[0m\n",
      "\u001b[33mProvided cores: 1 (use --cores to define parallelism)\u001b[0m\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tall\n",
      "\t2\tconvert_scripts_to_nb_and_run_using_jupytext\n",
      "\t1\tmake_archive\n",
      "\t4\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Fri Jan 22 16:47:47 2021]\u001b[0m\n",
      "\u001b[32mrule convert_scripts_to_nb_and_run_using_jupytext:\n",
      "    input: interactions_report_for_7c7a_E_B_6ah3_E_B.py\n",
      "    output: interactions_report_for_7c7a_E_B_6ah3_E_B.ipynb\n",
      "    jobid: 6\n",
      "    wildcards: details=7c7a_E_B_6ah3_E_B\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "[jupytext] Reading interactions_report_for_7c7a_E_B_6ah3_E_B.py in format py\n",
      "[jupytext] Executing notebook with kernel python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/bin/jupytext\", line 8, in <module>\n",
      "    sys.exit(jupytext())\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/jupytext/cli.py\", line 402, in jupytext\n",
      "    exit_code += jupytext_single_file(nb_file, args, log)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/jupytext/cli.py\", line 597, in jupytext_single_file\n",
      "    exec_proc.preprocess(notebook, resources=resources)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py\", line 405, in preprocess\n",
      "    nb, resources = super(ExecutePreprocessor, self).preprocess(nb, resources)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/nbconvert/preprocessors/base.py\", line 69, in preprocess\n",
      "    nb.cells[index], resources = self.preprocess_cell(cell, resources, index)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py\", line 448, in preprocess_cell\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, out)\n",
      "nbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "%run -i similarities_in_proteinprotein_interactions.py\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[0;32m~/notebooks/GSD/similarities_in_proteinprotein_interactions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m    235\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msuppress_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    236\u001b[0m     structure1_pdb_code, structure1_df = pdbsum_prot_interactions_list_to_df(\n",
      "\u001b[0;32m--> 237\u001b[0;31m         structure1_data_name, pickle_df=False, return_pdb_code=True)\n",
      "\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    239\u001b[0m     structure2_pdb_code, structure2_df = pdbsum_prot_interactions_list_to_df(\n",
      "\n",
      "\u001b[0;32m~/notebooks/GSD/pdbsum_prot_interactions_list_to_df.py\u001b[0m in \u001b[0;36mpdbsum_prot_interactions_list_to_df\u001b[0;34m(data_file, return_df, pickle_df, return_pdb_code)\u001b[0m\n",
      "\u001b[1;32m    219\u001b[0m     most_raw_data= raw_data_txt.split(\n",
      "\u001b[1;32m    220\u001b[0m         \u001b[0mpdb_code_delimiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 221\u001b[0;31m         typical_line_after_pdb,1)[1].split(split_on_string,1)[1]\n",
      "\u001b[0m\u001b[1;32m    222\u001b[0m     first_section_type = raw_data_txt.split(\n",
      "\u001b[1;32m    223\u001b[0m         \u001b[0mpdb_code_delimiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypical_line_after_pdb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range\n",
      "IndexError: list index out of range\n",
      "\n",
      "\u001b[32m[Fri Jan 22 16:47:52 2021]\u001b[0m\n",
      "\u001b[31mError in rule convert_scripts_to_nb_and_run_using_jupytext:\u001b[0m\n",
      "\u001b[31m    jobid: 6\u001b[0m\n",
      "\u001b[31m    output: interactions_report_for_7c7a_E_B_6ah3_E_B.ipynb\u001b[0m\n",
      "\u001b[31m    shell:\n",
      "        jupytext --to notebook --execute interactions_report_for_7c7a_E_B_6ah3_E_B.py --output interactions_report_for_7c7a_E_B_6ah3_E_B.ipynb;rm interactions_report_for_7c7a_E_B_6ah3_E_B.py\n",
      "        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)\u001b[0m\n",
      "\u001b[31m\u001b[0m\n",
      "\u001b[33mShutting down, this might take some time.\u001b[0m\n",
      "\u001b[31mExiting because a job execution failed. Look above for error message\u001b[0m\n",
      "\u001b[33mComplete log: /home/jovyan/notebooks/GSD/.snakemake/log/2021-01-22T164746.765286.snakemake.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!snakemake --cores 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that at present, I am seeing an issue with this step for this rule:\n",
    "    \n",
    "```python\n",
    "rule convert_scripts_to_nb_and_run_using_jupytext:\n",
    "    input: interactions_report_for_7c7a_E_B_6ah3_E_B.py\n",
    "    output: interactions_report_for_7c7a_E_B_6ah3_E_B.ipynb\n",
    "    jobid: 6\n",
    "    wildcards: details=7c7a_E_B_6ah3_E_B\n",
    "...\n",
    "nbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:\n",
    "------------------\n",
    "%run -i similarities_in_proteinprotein_interactions.py\n",
    "```\n",
    "\n",
    "I suspect this is 7c7a doesn't have an iteractions between chains E and B (Pop1p and Pop5p) and my script `similarities_in_proteinprotein_interactions.py` isn't set up to handle that gracefully yet. (Most likely than `differences_in_proteinprotein_interactions.py` has the same issue.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those knowlegeable with snakemake, I will say that I set the number of cores as one because I was finding with eight that occasionally a race condition would ensue where some of the auxillary scripts by notebooks would overwrite each other as they was being accessed by another notebook causing failures. Using one core avoids that hazard. I will add though that in most cases if you use multiple cores, you can easily get the additional files and a new archive made by running snakemake with your chosen number of cores again.\n",
    "\n",
    "I never saw a race hazard with my clean rule, and so if you want to quickly start over you can run `!snakemake --cores 8 clean`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step #3:** Verify the Jupyter notebooks with the reports were generated.  \n",
    "You can go to the dashboard and see the ouput of running snakemake. To do that click on the Jupyter logo in the upper left top of this notebook and on that page you'll look in  the notebooks directory and you should see files that begin with `interactions_report_` and end with `.ipynb`. You can examine some of them to insure all is as expected.\n",
    "\n",
    "If things seem to be working and you haven't run your data yet, run `!snakemake --cores 8 clean` in a cell to reset things, and then edit & save `int_matrix.txt` to have your information, and then run the `!snakemake --cores 1` step above, again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step #4:** If this was anyting other than the demonstration run, download the archive containing all the Jupyter notebooks bundled together.  \n",
    "For ease in downloading, all the created notebooks have been saved as a compressed archive so that you only need to retieve and keep track of one file. The file you are looking for begins with `interactions_report_nbs` in front of a date/time stamp and ends with `.tar.gz`. The snakemake run will actually highlight this archive towards the very bottom of the run, following the words 'Be sure to download'.  \n",
    "**Download that file from this remote, temporary session to your local computer.** You should see this archive file ending in `.tar.gz` on the dashboard. Toggle next to it to select it and then select `Download` to bring it from the remote Jupyterhub session to your computer. If you don't retieve that file and the session ends, you'll need to re-run to get the results again.\n",
    "\n",
    "You should be able to unpack that archive using your favorite software to extract compressed files. If that is proving difficult, you can always reopen a session like you did to run this series of notebooks and upload the archive and then run the following command in a Jupyter notebook cellk to unpack it:\n",
    "\n",
    "```bash\n",
    "!tar xzf interactions_report_nbs*\n",
    "```\n",
    "\n",
    "(If you are running that command on the command line, leave off the exclamation book.)\n",
    "You can then examine the files in the session or download the individual Jupyter notebooks similar to the advice on how to download the archive given above.\n",
    "\n",
    "In the next notebook in this series, [Making the multiple reports generated via snakemake clearer by adding protein names](Making%20the%20multiple%20reports%20generated%20via%20snakemake%20clearer%20by%20adding%20protein%20names.ipynb), I work through how to make the reports more human readable by swapping the chain designations with the actual names of the proteins. This is similar to making the report more human readable that was discussed at the bottom of the previous notebook, [Using PDBsum data to highlight changes in protein-protein interactions](Using%20PDBsum%20data%20to%20highlight%20changes%20in%20protein-protein%20interactions.ipynb); however, it will be done to all the notebooks at once based on the file name beginning with `interactions_report_for_` and ending with `.ipynb`.\n",
    "\n",
    "-----\n",
    "\n",
    "Please continue on with the next notebook in this series, [Making the multiple reports generated via snakemake clearer by adding protein names](Making%20the%20multiple%20reports%20generated%20via%20snakemake%20clearer%20by%20adding%20protein%20names.ipynb).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Enjoy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def executeSomething():\n",
    "    #code here\n",
    "    print ('.')\n",
    "    time.sleep(480) #60 seconds times 8 minutes\n",
    "\n",
    "while True:\n",
    "    executeSomething()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
